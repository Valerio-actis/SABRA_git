{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81011325",
   "metadata": {},
   "source": [
    "## final comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "148daf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /home/vale/miniconda3/envs/myenv/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /home/vale/.local/lib/python3.10/site-packages (from seaborn) (1.26.1)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/vale/miniconda3/envs/myenv/lib/python3.10/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /home/vale/miniconda3/envs/myenv/lib/python3.10/site-packages (from seaborn) (3.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/vale/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/vale/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/vale/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/vale/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/vale/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/vale/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/vale/miniconda3/envs/myenv/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/vale/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/vale/miniconda3/envs/myenv/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/vale/miniconda3/envs/myenv/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/vale/miniconda3/envs/myenv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install seaborn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "def parse_filename(filename):\n",
    "    \"\"\"Parse filename to extract parameters.\"\"\"\n",
    "    params = {}\n",
    "    \n",
    "    # Check if it's a batch approach file\n",
    "    if 'W_final_' in filename:\n",
    "        params['approach'] = 'batch'\n",
    "        # Extract parameters using regex\n",
    "        patterns = {\n",
    "            'shell': r'shell(\\d+)',\n",
    "            'nbatch': r'nbatch(\\d+)',\n",
    "            'tstart': r'tstart(\\d+)',\n",
    "            'tend': r'tend(\\d+)',\n",
    "            'allint': r'allint(True|False)',\n",
    "            'subtractD': r'subtractD(True|False)',\n",
    "            'knorm': r'knorm(True|False)',\n",
    "            'lassoCV': r'lassoCVTrue|lassoCV(True|False)',\n",
    "            'lambda': r'lambda([\\d.]+)',\n",
    "            'randombatch': r'randombatch(True|False)',\n",
    "            'sigthresh': r'sigthresh([\\d.]+)',\n",
    "            'minocc': r'minocc(\\d+)'\n",
    "        }\n",
    "    else:\n",
    "        params['approach'] = 'single'\n",
    "        # Extract parameters for single approach\n",
    "        patterns = {\n",
    "            'shell': r'shell(\\d+)',\n",
    "            'nn': r'nn(\\d+)',\n",
    "            'tstart': r'tstart(\\d+)',\n",
    "            'lassoCV': r'lassoCVTrue|lassoCV(True|False)',\n",
    "            'subtractD': r'subtractD(True|False)',\n",
    "            'knorm': r'knorm(True|False)'\n",
    "        }\n",
    "    \n",
    "    for param, pattern in patterns.items():\n",
    "        match = re.search(pattern, filename)\n",
    "        if match:\n",
    "            if param in ['shell', 'nbatch', 'tstart', 'tend', 'nn', 'minocc']:\n",
    "                params[param] = int(match.group(1))\n",
    "            elif param in ['lambda', 'sigthresh']:\n",
    "                params[param] = float(match.group(1))\n",
    "            elif param == 'lassoCV':\n",
    "                params[param] = 'True' in match.group(0)\n",
    "            else:\n",
    "                params[param] = match.group(1) == 'True'\n",
    "    \n",
    "    return params\n",
    "\n",
    "def load_coefficient_data(filepath):\n",
    "    \"\"\"Load coefficient data from file.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        return data\n",
    "    except:\n",
    "        # Try numpy load if pickle fails\n",
    "        try:\n",
    "            data = np.load(filepath, allow_pickle=True)\n",
    "            return data\n",
    "        except:\n",
    "            print(f\"Could not load {filepath}\")\n",
    "            return None\n",
    "\n",
    "def compare_sabra_approaches(data_directory, nn=20, max_shells=None, \n",
    "                           filter_params=None, save_plots=True):\n",
    "    \"\"\"\n",
    "    Compare SABRA regression approaches and visualize results.\n",
    "    \n",
    "    Args:\n",
    "        data_directory: Path to directory containing W files\n",
    "        nn: Total number of shells\n",
    "        max_shells: Maximum number of shells to analyze (None for all)\n",
    "        filter_params: Dict of parameter filters (e.g., {'subtractD': False})\n",
    "        save_plots: Whether to save plots to files\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get all W files\n",
    "    files = [f for f in os.listdir(data_directory) if f.startswith('W_')]\n",
    "    \n",
    "    # Parse files and group by parameters\n",
    "    results = defaultdict(list)\n",
    "    \n",
    "    for filename in files:\n",
    "        params = parse_filename(filename)\n",
    "        \n",
    "        # Apply filters if specified\n",
    "        if filter_params:\n",
    "            skip = False\n",
    "            for key, value in filter_params.items():\n",
    "                if key in params and params[key] != value:\n",
    "                    skip = True\n",
    "                    break\n",
    "            if skip:\n",
    "                continue\n",
    "        \n",
    "        # Load coefficient data\n",
    "        filepath = os.path.join(data_directory, filename)\n",
    "        W_data = load_coefficient_data(filepath)\n",
    "        \n",
    "        if W_data is None:\n",
    "            continue\n",
    "        \n",
    "        # Handle both single W arrays and lists of W arrays\n",
    "        if W_data.shape==(50,1,1600):\n",
    "            # For single approach, use the last W (highest sample size)\n",
    "            W = W_data[-1] if params['approach'] == 'single' else W_data[0]\n",
    "            print(f\"Using last W for single approach: {W.shape}\")\n",
    "        else:\n",
    "            W = W_data\n",
    "        \n",
    "        # Check the structure of your data\n",
    "        print(f\"Type of W: {type(W)}\")\n",
    "        print(f\"Shape: {W.shape if hasattr(W, 'shape') else 'No shape'}\")\n",
    "        print(f\"First few elements: {W[:5] if hasattr(W, '__getitem__') else W}\")\n",
    "        # Analyze coefficients\n",
    "        shell_idx = params['shell'] - 1  # Convert to 0-based indexing\n",
    "        \n",
    "        if max_shells and shell_idx >= max_shells:\n",
    "            continue\n",
    "            \n",
    "        analysis = analyze_coefficients(W, nn, shell_idx, \n",
    "                                      params.get('subtractD', False),\n",
    "                                      params.get('allint', True))\n",
    "        \n",
    "        # Store results\n",
    "        result_key = tuple(sorted([(k, v) for k, v in params.items() \n",
    "                                 if k not in ['shell', 'tstart', 'tend']]))\n",
    "        \n",
    "        results[result_key].append({\n",
    "            'shell': params['shell'],\n",
    "            'approach': params['approach'],\n",
    "            'analysis': analysis,\n",
    "            'params': params\n",
    "        })\n",
    "    \n",
    "    # Create visualizations for each parameter combination\n",
    "    for param_combo, shell_results in results.items():\n",
    "        create_comparison_plots(shell_results, param_combo, nn, save_plots)\n",
    "\n",
    "    create_grouped_figures(results, nn, save_plots)\n",
    "\n",
    "def analyze_coefficients(W, nn, shell_idx, subtractD, allint=True):\n",
    "    \"\"\"Analyze coefficients to separate expected vs unexpected interactions.\"\"\"\n",
    "    \n",
    "    # Handle different W formats\n",
    "    if isinstance(W, (list, tuple)):\n",
    "        # If W is a list/tuple, take the first element or flatten\n",
    "        if len(W) > 0:\n",
    "            W_array = np.array(W[0]) if hasattr(W[0], '__len__') else np.array(W)\n",
    "        else:\n",
    "            return {'expected_sum': 0, 'unexpected_sum': 0, 'expected_dissipation_sum': 0, 'unexpected_dissipation_sum': 0,\n",
    "                   'expected_count': 0, 'unexpected_count': 0, 'expected_dissipation_count': 0, 'unexpected_dissipation_count': 0,\n",
    "                   'total_sum': 0, 'expected_percentage': 0, 'unexpected_percentage': 0,\n",
    "                   'expected_dissipation_percentage': 0, 'unexpected_dissipation_percentage': 0}\n",
    "    else:\n",
    "        W_array = np.array(W)\n",
    "    \n",
    "    # Flatten if multidimensional\n",
    "    if W_array.ndim > 1:\n",
    "        W_array = W_array.flatten()\n",
    "    \n",
    "    # Get expected interactions for this shell\n",
    "    expected_interactions = get_expected_interactions_single_shell(nn, shell_idx)\n",
    "    \n",
    "    # Build feature mapping\n",
    "    include_dissipation = not subtractD\n",
    "    feature_mapping = build_feature_mapping(nn, use_all_interactions=allint, \n",
    "                                          include_dissipation=include_dissipation)\n",
    "    \n",
    "    expected_coeff_sum = 0\n",
    "    unexpected_coeff_sum = 0\n",
    "    expected_count = 0\n",
    "    unexpected_count = 0\n",
    "    \n",
    "    expected_dissipation_sum = 0\n",
    "    unexpected_dissipation_sum = 0\n",
    "    expected_dissipation_count = 0\n",
    "    unexpected_dissipation_count = 0\n",
    "    \n",
    "    # Analyze each coefficient\n",
    "    for idx, coeff in enumerate(W_array):\n",
    "        # Handle scalar vs array coefficients\n",
    "        if hasattr(coeff, '__len__') and len(coeff) > 1:\n",
    "            # If coeff is an array, take its magnitude\n",
    "            coeff_val = np.linalg.norm(coeff)\n",
    "        else:\n",
    "            coeff_val = float(coeff)\n",
    "            \n",
    "        if abs(coeff_val) < 1e-10:  # Skip essentially zero coefficients\n",
    "            continue\n",
    "            \n",
    "        if idx in feature_mapping:\n",
    "            interaction = feature_mapping[idx]\n",
    "            \n",
    "            if len(interaction) == 2 and interaction[1] == 'dissipation':\n",
    "                # Dissipation term - check if it's from the current shell\n",
    "                dissipation_shell = interaction[0]\n",
    "                if dissipation_shell == shell_idx:\n",
    "                    expected_dissipation_sum += abs(coeff_val)\n",
    "                    expected_dissipation_count += 1\n",
    "                else:\n",
    "                    unexpected_dissipation_sum += abs(coeff_val)\n",
    "                    unexpected_dissipation_count += 1\n",
    "            else:\n",
    "                # Interaction term\n",
    "                i, j, int_type = interaction\n",
    "                \n",
    "                if (i, j, int_type) in expected_interactions:\n",
    "                    expected_coeff_sum += abs(coeff_val)\n",
    "                    expected_count += 1\n",
    "                else:\n",
    "                    unexpected_coeff_sum += abs(coeff_val)\n",
    "                    unexpected_count += 1\n",
    "    \n",
    "    total_sum = expected_coeff_sum + unexpected_coeff_sum + expected_dissipation_sum + unexpected_dissipation_sum\n",
    "    \n",
    "    return {\n",
    "        'expected_sum': expected_coeff_sum,\n",
    "        'unexpected_sum': unexpected_coeff_sum,\n",
    "        'expected_dissipation_sum': expected_dissipation_sum,\n",
    "        'unexpected_dissipation_sum': unexpected_dissipation_sum,\n",
    "        'expected_count': expected_count,\n",
    "        'unexpected_count': unexpected_count,\n",
    "        'expected_dissipation_count': expected_dissipation_count,\n",
    "        'unexpected_dissipation_count': unexpected_dissipation_count,\n",
    "        'total_sum': total_sum,\n",
    "        'expected_percentage': (expected_coeff_sum / total_sum * 100) if total_sum > 0 else 0,\n",
    "        'unexpected_percentage': (unexpected_coeff_sum / total_sum * 100) if total_sum > 0 else 0,\n",
    "        'expected_dissipation_percentage': (expected_dissipation_sum / total_sum * 100) if total_sum > 0 else 0,\n",
    "        'unexpected_dissipation_percentage': (unexpected_dissipation_sum / total_sum * 100) if total_sum > 0 else 0\n",
    "    }\n",
    "\n",
    "def plot_interaction_counts(ax, single_results, batch_results, subtractD=False):\n",
    "    \"\"\"Plot number of interactions by type with improved visualization and high contrast colors.\"\"\"\n",
    "    shells_single = [r['shell'] for r in single_results]\n",
    "    shells_batch = [r['shell'] for r in batch_results]\n",
    "    \n",
    "    # Get all shells for x-axis\n",
    "    all_shells = sorted(set(shells_single + shells_batch))\n",
    "    \n",
    "    expected_single = [r['analysis']['expected_count'] for r in single_results]\n",
    "    unexpected_single = [r['analysis']['unexpected_count'] for r in single_results]\n",
    "    exp_dissipation_single = [r['analysis']['expected_dissipation_count'] for r in single_results]\n",
    "    unexp_dissipation_single = [r['analysis']['unexpected_dissipation_count'] for r in single_results]\n",
    "    \n",
    "    expected_batch = [r['analysis']['expected_count'] for r in batch_results]\n",
    "    unexpected_batch = [r['analysis']['unexpected_count'] for r in batch_results]\n",
    "    exp_dissipation_batch = [r['analysis']['expected_dissipation_count'] for r in batch_results]\n",
    "    unexp_dissipation_batch = [r['analysis']['unexpected_dissipation_count'] for r in batch_results]\n",
    "    \n",
    "    width = 0.35\n",
    "    \n",
    "    x_single = np.array(shells_single) - width/2\n",
    "    x_batch = np.array(shells_batch) + width/2\n",
    "    \n",
    "    # High contrast colors\n",
    "    colors = {\n",
    "        'expected_single': \"#007191\",      # Dark Green\n",
    "        'exp_dissip_single': '#32CD32',    # Lime Green\n",
    "        'unexpected_single': '#8B0000',    # Dark Red\n",
    "        'unexp_dissip_single': '#FF4500',  # Orange Red\n",
    "        'expected_batch': '#000080',       # Navy Blue\n",
    "        'exp_dissip_batch': \"#E1E141\",     # Royal Blue\n",
    "        'unexpected_batch': '#800080',     # Purple\n",
    "        'unexp_dissip_batch': \"#FF7B00\"    # Orchid\n",
    "    }\n",
    "    \n",
    "    if single_results:\n",
    "        ax.bar(x_single, expected_single, width/2, label='Expected (Single)', \n",
    "               color=colors['expected_single'], alpha=0.9)\n",
    "        ax.bar(x_single + width/2, exp_dissipation_single, width/2, \n",
    "               label='Expected Dissipation (Single)', color=colors['exp_dissip_single'], alpha=0.9)\n",
    "        \n",
    "        bottom_single_1 = np.array(expected_single)\n",
    "        bottom_single_2 = np.array(exp_dissipation_single)\n",
    "        ax.bar(x_single, unexpected_single, width/2, bottom=bottom_single_1,\n",
    "               label='Unexpected (Single)', color=colors['unexpected_single'], alpha=0.9)\n",
    "        ax.bar(x_single + width/2, unexp_dissipation_single, width/2, bottom=bottom_single_2,\n",
    "               label='Unexpected Dissipation (Single)', color=colors['unexp_dissip_single'], alpha=0.9)\n",
    "    \n",
    "    if batch_results:\n",
    "        ax.bar(x_batch, expected_batch, width/2, label='Expected (Batch)', \n",
    "               color=colors['expected_batch'], alpha=0.9)\n",
    "        ax.bar(x_batch + width/2, exp_dissipation_batch, width/2,\n",
    "               label='Expected Dissipation (Batch)', color=colors['exp_dissip_batch'], alpha=0.9)\n",
    "        \n",
    "        bottom_batch_1 = np.array(expected_batch)\n",
    "        bottom_batch_2 = np.array(exp_dissipation_batch)\n",
    "        ax.bar(x_batch, unexpected_batch, width/2, bottom=bottom_batch_1,\n",
    "               label='Unexpected (Batch)', color=colors['unexpected_batch'], alpha=0.9)\n",
    "        ax.bar(x_batch + width/2, unexp_dissipation_batch, width/2, bottom=bottom_batch_2,\n",
    "               label='Unexpected Dissipation (Batch)', color=colors['unexp_dissip_batch'], alpha=0.9)\n",
    "    \n",
    "    # Add horizontal lines\n",
    "    ax.axhline(y=6, color='red', linestyle='--', linewidth=2, alpha=0.7, label='Maximum Expected Interactions (6)')\n",
    "    \n",
    "    # Add line at y=1 only when subtractD is False (dissipation terms are included)\n",
    "    if not subtractD:\n",
    "        ax.axhline(y=1, color='orange', linestyle='--', linewidth=2, alpha=0.7, \n",
    "                  label='Maximum Expected Dissipation (1)')\n",
    "    \n",
    "    ax.set_xlabel('Shell Number')\n",
    "    ax.set_ylabel('Number of Interactions')\n",
    "    ax.set_xticks(all_shells)\n",
    "    ax.set_xticklabels([str(int(s)) for s in all_shells])\n",
    "    # Change legend location to overlap with plot\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "def plot_total_magnitude(ax, single_results, batch_results):\n",
    "    \"\"\"Plot total coefficient magnitude.\"\"\"\n",
    "    shells_single = [r['shell'] for r in single_results]\n",
    "    shells_batch = [r['shell'] for r in batch_results]\n",
    "    \n",
    "    magnitudes_single = [r['analysis']['total_sum'] for r in single_results]\n",
    "    magnitudes_batch = [r['analysis']['total_sum'] for r in batch_results]\n",
    "    \n",
    "    # Get all shell numbers for x-axis\n",
    "    all_shells = sorted(set(shells_single + shells_batch))\n",
    "    \n",
    "    if single_results:\n",
    "        ax.semilogy(shells_single, magnitudes_single, 'o-', label='Single Approach', \n",
    "                   color='green', linewidth=2, markersize=8)\n",
    "    \n",
    "    if batch_results:\n",
    "        ax.semilogy(shells_batch, magnitudes_batch, 's-', label='Batch Approach', \n",
    "                   color='blue', linewidth=2, markersize=8)\n",
    "    \n",
    "    ax.set_xlabel('Shell Number')\n",
    "    ax.set_ylabel('Total Coefficient Magnitude (log scale)')\n",
    "    # Set explicit x-axis ticks for all shell numbers as integers\n",
    "    ax.set_xticks(all_shells)\n",
    "    ax.set_xticklabels([str(int(shell)) for shell in all_shells])\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "def create_summary_table(single_results, batch_results, param_dict, save_plots):\n",
    "    \"\"\"Create and display a quantitative summary table.\"\"\"\n",
    "    \n",
    "    # Combine all results\n",
    "    all_results = single_results + batch_results\n",
    "    shells = sorted(set([r['shell'] for r in all_results]))\n",
    "    \n",
    "    # Create summary data\n",
    "    summary_data = []\n",
    "    \n",
    "    for shell in shells:\n",
    "        # Single approach data\n",
    "        single_data = [r for r in single_results if r['shell'] == shell]\n",
    "        batch_data = [r for r in batch_results if r['shell'] == shell]\n",
    "        \n",
    "        row = {'Shell': int(shell)}\n",
    "        \n",
    "        if single_data:\n",
    "            analysis = single_data[0]['analysis']\n",
    "            row.update({\n",
    "                'Single_Expected': analysis['expected_count'],\n",
    "                'Single_Unexpected': analysis['unexpected_count'],\n",
    "                'Single_ExpDissip': analysis['expected_dissipation_count'],\n",
    "                'Single_UnexpDissip': analysis['unexpected_dissipation_count'],\n",
    "                'Single_TotalMag': f\"{analysis['total_sum']:.2e}\",\n",
    "                'Single_ExpectedRatio': f\"{analysis['expected_count']/(analysis['expected_count']+analysis['unexpected_count'])*100:.1f}%\" if (analysis['expected_count']+analysis['unexpected_count']) > 0 else \"N/A\"\n",
    "            })\n",
    "        else:\n",
    "            row.update({\n",
    "                'Single_Expected': 0, 'Single_Unexpected': 0, 'Single_ExpDissip': 0, \n",
    "                'Single_UnexpDissip': 0, 'Single_TotalMag': \"0.00e+00\", 'Single_ExpectedRatio': \"N/A\"\n",
    "            })\n",
    "        \n",
    "        if batch_data:\n",
    "            analysis = batch_data[0]['analysis']\n",
    "            row.update({\n",
    "                'Batch_Expected': analysis['expected_count'],\n",
    "                'Batch_Unexpected': analysis['unexpected_count'],\n",
    "                'Batch_ExpDissip': analysis['expected_dissipation_count'],\n",
    "                'Batch_UnexpDissip': analysis['unexpected_dissipation_count'],\n",
    "                'Batch_TotalMag': f\"{analysis['total_sum']:.2e}\",\n",
    "                'Batch_ExpectedRatio': f\"{analysis['expected_count']/(analysis['expected_count']+analysis['unexpected_count'])*100:.1f}%\" if (analysis['expected_count']+analysis['unexpected_count']) > 0 else \"N/A\"\n",
    "            })\n",
    "        else:\n",
    "            row.update({\n",
    "                'Batch_Expected': 0, 'Batch_Unexpected': 0, 'Batch_ExpDissip': 0, \n",
    "                'Batch_UnexpDissip': 0, 'Batch_TotalMag': \"0.00e+00\", 'Batch_ExpectedRatio': \"N/A\"\n",
    "            })\n",
    "        \n",
    "        summary_data.append(row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Display table\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"QUANTITATIVE SUMMARY\")\n",
    "    print(f\"Parameters: {', '.join([f'{k}={v}' for k, v in param_dict.items()])}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(df.to_string(index=False))\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Save table if requested\n",
    "    if save_plots:\n",
    "        param_str = '_'.join([f\"{k}{v}\" for k, v in param_dict.items()])\n",
    "        filename = f\"sabra_summary_table_{param_str}.csv\"\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Saved summary table: {filename}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def plot_expected_ratio(ax, single_results, batch_results):\n",
    "    \"\"\"Plot ratio of expected to total interactions with improved formatting.\"\"\"\n",
    "    shells_single = [r['shell'] for r in single_results]\n",
    "    shells_batch = [r['shell'] for r in batch_results]\n",
    "    \n",
    "    ratios_single = []\n",
    "    ratios_batch = []\n",
    "    \n",
    "    for r in single_results:\n",
    "        total_interactions = r['analysis']['expected_count'] + r['analysis']['unexpected_count']\n",
    "        ratio = r['analysis']['expected_count'] / total_interactions if total_interactions > 0 else 0\n",
    "        ratios_single.append(ratio)\n",
    "    \n",
    "    for r in batch_results:\n",
    "        total_interactions = r['analysis']['expected_count'] + r['analysis']['unexpected_count']\n",
    "        ratio = r['analysis']['expected_count'] / total_interactions if total_interactions > 0 else 0\n",
    "        ratios_batch.append(ratio)\n",
    "    \n",
    "    if single_results:\n",
    "        ax.plot(shells_single, ratios_single, 'o-', label='Single Approach', \n",
    "               color='#2E8B57', linewidth=2, markersize=6)\n",
    "    \n",
    "    if batch_results:\n",
    "        ax.plot(shells_batch, ratios_batch, 's-', label='Batch Approach', \n",
    "               color='#1E3A8A', linewidth=2, markersize=6)\n",
    "    \n",
    "    ax.set_xlabel('Shell Number')\n",
    "    ax.set_ylabel('Expected Interactions / Total Interactions')\n",
    "    ax.set_ylim(-0.05, 1.05)  # Add padding to avoid overlap with borders\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Improve tick spacing if needed\n",
    "    if shells_single or shells_batch:\n",
    "        all_shells = sorted(set(shells_single + shells_batch))\n",
    "        ax.set_xticks(all_shells)\n",
    "        ax.set_xticklabels([str(int(s)) for s in all_shells])\n",
    "\n",
    "def plot_coefficient_percentages(ax, single_results, batch_results):\n",
    "    \"\"\"Plot percentage of coefficient magnitude by type with improved formatting.\"\"\"\n",
    "    shells = sorted(set([r['shell'] for r in single_results + batch_results]))\n",
    "    \n",
    "    # Prepare data\n",
    "    expected_perc_single = []\n",
    "    unexpected_perc_single = []\n",
    "    exp_dissipation_perc_single = []\n",
    "    unexp_dissipation_perc_single = []\n",
    "    expected_perc_batch = []\n",
    "    unexpected_perc_batch = []\n",
    "    exp_dissipation_perc_batch = []\n",
    "    unexp_dissipation_perc_batch = []\n",
    "    \n",
    "    for shell in shells:\n",
    "        # Single approach\n",
    "        single_data = [r for r in single_results if r['shell'] == shell]\n",
    "        if single_data:\n",
    "            analysis = single_data[0]['analysis']\n",
    "            expected_perc_single.append(analysis['expected_percentage'])\n",
    "            unexpected_perc_single.append(analysis['unexpected_percentage'])\n",
    "            exp_dissipation_perc_single.append(analysis['expected_dissipation_percentage'])\n",
    "            unexp_dissipation_perc_single.append(analysis['unexpected_dissipation_percentage'])\n",
    "        else:\n",
    "            expected_perc_single.append(0)\n",
    "            unexpected_perc_single.append(0)\n",
    "            exp_dissipation_perc_single.append(0)\n",
    "            unexp_dissipation_perc_single.append(0)\n",
    "        \n",
    "        # Batch approach\n",
    "        batch_data = [r for r in batch_results if r['shell'] == shell]\n",
    "        if batch_data:\n",
    "            analysis = batch_data[0]['analysis']\n",
    "            expected_perc_batch.append(analysis['expected_percentage'])\n",
    "            unexpected_perc_batch.append(analysis['unexpected_percentage'])\n",
    "            exp_dissipation_perc_batch.append(analysis['expected_dissipation_percentage'])\n",
    "            unexp_dissipation_perc_batch.append(analysis['unexpected_dissipation_percentage'])\n",
    "        else:\n",
    "            expected_perc_batch.append(0)\n",
    "            unexpected_perc_batch.append(0)\n",
    "            exp_dissipation_perc_batch.append(0)\n",
    "            unexp_dissipation_perc_batch.append(0)\n",
    "    \n",
    "    x = np.arange(len(shells))\n",
    "    width = 0.35\n",
    "    \n",
    "    # High contrast colors\n",
    "    colors = {\n",
    "        'expected_single': '#006400',\n",
    "        'exp_dissip_single': '#32CD32',\n",
    "        'unexpected_single': '#8B0000',\n",
    "        'unexp_dissip_single': \"#FF4F0F\",\n",
    "        'expected_batch': '#000080',\n",
    "        'exp_dissip_batch': '#4169E1',\n",
    "        'unexpected_batch': '#800080',\n",
    "        'unexp_dissip_batch': '#DA70D6'\n",
    "    }\n",
    "    \n",
    "    # Stacked bars\n",
    "    ax.bar(x - width/2, expected_perc_single, width, label='Expected (Single)', \n",
    "           color=colors['expected_single'], alpha=0.9)\n",
    "    ax.bar(x - width/2, exp_dissipation_perc_single, width, bottom=expected_perc_single,\n",
    "           label='Expected Dissipation (Single)', color=colors['exp_dissip_single'], alpha=0.9)\n",
    "    \n",
    "    bottom_single_1 = np.array(expected_perc_single) + np.array(exp_dissipation_perc_single)\n",
    "    ax.bar(x - width/2, unexpected_perc_single, width, bottom=bottom_single_1,\n",
    "           label='Unexpected (Single)', color=colors['unexpected_single'], alpha=0.9)\n",
    "    \n",
    "    bottom_single_2 = bottom_single_1 + np.array(unexpected_perc_single)\n",
    "    ax.bar(x - width/2, unexp_dissipation_perc_single, width, bottom=bottom_single_2,\n",
    "           label='Unexpected Dissipation (Single)', color=colors['unexp_dissip_single'], alpha=0.9)\n",
    "    \n",
    "    ax.bar(x + width/2, expected_perc_batch, width, label='Expected (Batch)', \n",
    "           color=colors['expected_batch'], alpha=0.9)\n",
    "    ax.bar(x + width/2, exp_dissipation_perc_batch, width, bottom=expected_perc_batch,\n",
    "           label='Expected Dissipation (Batch)', color=colors['exp_dissip_batch'], alpha=0.9)\n",
    "    \n",
    "    bottom_batch_1 = np.array(expected_perc_batch) + np.array(exp_dissipation_perc_batch)\n",
    "    ax.bar(x + width/2, unexpected_perc_batch, width, bottom=bottom_batch_1,\n",
    "           label='Unexpected (Batch)', color=colors['unexpected_batch'], alpha=0.9)\n",
    "    \n",
    "    bottom_batch_2 = bottom_batch_1 + np.array(unexpected_perc_batch)\n",
    "    ax.bar(x + width/2, unexp_dissipation_perc_batch, width, bottom=bottom_batch_2,\n",
    "           label='Unexpected Dissipation (Batch)', color=colors['unexp_dissip_batch'], alpha=0.9)\n",
    "    \n",
    "    ax.set_xlabel('Shell Number')\n",
    "    ax.set_ylabel('Percentage of Total Magnitude')\n",
    "    ax.set_ylim(-2, 102)  # Add padding to avoid overlap with borders\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([str(int(shell)) for shell in shells])\n",
    "    ax.legend(loc='lower left', borderaxespad=0.)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "def create_comparison_plots(shell_results, param_combo, nn, save_plots):\n",
    "    \"\"\"Create comparison plots for a specific parameter combination without showing them.\"\"\"\n",
    "    \n",
    "    # Group by approach\n",
    "    single_results = [r for r in shell_results if r['approach'] == 'single']\n",
    "    batch_results = [r for r in shell_results if r['approach'] == 'batch']\n",
    "    \n",
    "    # Sort by shell number\n",
    "    single_results.sort(key=lambda x: x['shell'])\n",
    "    batch_results.sort(key=lambda x: x['shell'])\n",
    "    \n",
    "    # Extract parameter info for title\n",
    "    param_dict = dict(param_combo)\n",
    "    title_parts = []\n",
    "    for key, value in param_dict.items():\n",
    "        if key in ['subtractD', 'knorm', 'lassoCV', 'allint']:\n",
    "            title_parts.append(f\"{key}={value}\")\n",
    "    \n",
    "    # Create individual plots without showing them\n",
    "    plt.ioff()  # Turn off interactive mode\n",
    "    \n",
    "    # Plot 1: Number of interactions by type\n",
    "    fig1, ax1 = plt.subplots(figsize=(12, 8))\n",
    "    plot_interaction_counts(ax1, single_results, batch_results, param_dict.get('subtractD', False))\n",
    "    ax1.set_title(f'Number of Interactions Learned\\n{\", \".join(title_parts)}', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    if save_plots:\n",
    "        param_str = '_'.join([f\"{k}{v}\" for k, v in param_dict.items()])\n",
    "        filename1 = f\"sabra_interaction_counts_{param_str}.png\"\n",
    "        fig1.savefig(filename1, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved plot: {filename1}\")\n",
    "    \n",
    "    plt.close(fig1)  # Close instead of show\n",
    "    \n",
    "    # Plot 2: Percentage of coefficient magnitude\n",
    "    fig2, ax2 = plt.subplots(figsize=(12, 8))\n",
    "    plot_coefficient_percentages(ax2, single_results, batch_results)\n",
    "    ax2.set_title(f'Coefficient Magnitude Distribution\\n{\", \".join(title_parts)}', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    if save_plots:\n",
    "        filename2 = f\"sabra_coefficient_percentages_{param_str}.png\"\n",
    "        fig2.savefig(filename2, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved plot: {filename2}\")\n",
    "    \n",
    "    plt.close(fig2)\n",
    "    \n",
    "    # Plot 3: Expected vs Unexpected ratio\n",
    "    fig3, ax3 = plt.subplots(figsize=(12, 8))\n",
    "    plot_expected_ratio(ax3, single_results, batch_results)\n",
    "    ax3.set_title(f'Expected vs Unexpected Interaction Ratio\\n{\", \".join(title_parts)}', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    if save_plots:\n",
    "        filename3 = f\"sabra_expected_ratio_{param_str}.png\"\n",
    "        fig3.savefig(filename3, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved plot: {filename3}\")\n",
    "    \n",
    "    plt.close(fig3)\n",
    "    \n",
    "    # Plot 4: Total coefficient magnitude\n",
    "    fig4, ax4 = plt.subplots(figsize=(12, 8))\n",
    "    plot_total_magnitude(ax4, single_results, batch_results)\n",
    "    ax4.set_title(f'Total Coefficient Magnitude\\n{\", \".join(title_parts)}', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    if save_plots:\n",
    "        filename4 = f\"sabra_total_magnitude_{param_str}.png\"\n",
    "        fig4.savefig(filename4, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved plot: {filename4}\")\n",
    "    \n",
    "    plt.close(fig4)\n",
    "    \n",
    "    plt.ion()  # Turn interactive mode back on\n",
    "    \n",
    "    # Create and display quantitative summary table\n",
    "    create_summary_table(single_results, batch_results, param_dict, save_plots)\n",
    "\n",
    "\n",
    "def plot_interaction_counts_expected(ax, single_results, batch_results, subtractD=False):\n",
    "    \"\"\"Plot number of expected interactions/dissipations only.\"\"\"\n",
    "    shells_single = [r['shell'] for r in single_results]\n",
    "    shells_batch = [r['shell'] for r in batch_results]\n",
    "    \n",
    "    # Get all shells for x-axis\n",
    "    all_shells = sorted(set(shells_single + shells_batch))\n",
    "    \n",
    "    expected_single = [r['analysis']['expected_count'] for r in single_results]\n",
    "    exp_dissipation_single = [r['analysis']['expected_dissipation_count'] for r in single_results]\n",
    "    \n",
    "    expected_batch = [r['analysis']['expected_count'] for r in batch_results]\n",
    "    exp_dissipation_batch = [r['analysis']['expected_dissipation_count'] for r in batch_results]\n",
    "    \n",
    "    width = 0.35\n",
    "    \n",
    "    x_single = np.array(shells_single) - width/2\n",
    "    x_batch = np.array(shells_batch) + width/2\n",
    "    \n",
    "    # High contrast colors\n",
    "    colors = {\n",
    "        'expected_single': \"#007191\",      # Dark Green\n",
    "        'exp_dissip_single': '#32CD32',    # Lime Green\n",
    "        'expected_batch': '#000080',       # Navy Blue\n",
    "        'exp_dissip_batch': \"#E1E141\"      # Royal Blue\n",
    "    }\n",
    "    \n",
    "    if single_results:\n",
    "        ax.bar(x_single, expected_single, width/2, label='Expected (Single)', \n",
    "               color=colors['expected_single'], alpha=0.9)\n",
    "        ax.bar(x_single + width/2, exp_dissipation_single, width/2, \n",
    "               label='Expected Dissipation (Single)', color=colors['exp_dissip_single'], alpha=0.9)\n",
    "    \n",
    "    if batch_results:\n",
    "        ax.bar(x_batch, expected_batch, width/2, label='Expected (Batch)', \n",
    "               color=colors['expected_batch'], alpha=0.9)\n",
    "        ax.bar(x_batch + width/2, exp_dissipation_batch, width/2,\n",
    "               label='Expected Dissipation (Batch)', color=colors['exp_dissip_batch'], alpha=0.9)\n",
    "    \n",
    "    # Add horizontal lines\n",
    "    ax.axhline(y=6, color='red', linestyle='--', linewidth=2, alpha=0.7, label='Maximum Expected Interactions (6)')\n",
    "    \n",
    "    # Add line at y=1 only when subtractD is False (dissipation terms are included)\n",
    "    if not subtractD:\n",
    "        ax.axhline(y=1, color='orange', linestyle='--', linewidth=2, alpha=0.7, \n",
    "                  label='Maximum Expected Dissipation (1)')\n",
    "    \n",
    "    ax.set_xlabel('Shell Number')\n",
    "    ax.set_ylabel('Number of Expected Interactions')\n",
    "    ax.set_xticks(all_shells)\n",
    "    ax.set_xticklabels([str(int(s)) for s in all_shells])\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "def create_grouped_figures(results, nn, save_plots):\n",
    "    \"\"\"Create separate grouped figures for each parameter combination, showing batch and single results separately.\"\"\"\n",
    "    \n",
    "    plt.ioff()  # Turn off interactive mode\n",
    "    \n",
    "    for param_combo, shell_results in results.items():\n",
    "        param_dict = dict(param_combo)\n",
    "        \n",
    "        single_results = [r for r in shell_results if r['approach'] == 'single']\n",
    "        batch_results = [r for r in shell_results if r['approach'] == 'batch']\n",
    "        \n",
    "        single_results.sort(key=lambda x: x['shell'])\n",
    "        batch_results.sort(key=lambda x: x['shell'])\n",
    "\n",
    "\n",
    "        title_parts = [f\"{k}={v}\" for k, v in param_dict.items() \n",
    "                      if k in ['subtractD', 'knorm', 'lassoCV', 'allint']]\n",
    "        param_str = '_'.join([f\"{k}{v}\" for k, v in param_dict.items()])\n",
    "        \n",
    "        # Create batch approach figure if batch results exist\n",
    "        if batch_results:\n",
    "            fig_batch, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 10))\n",
    "            \n",
    "            # Interaction counts\n",
    "            plot_interaction_counts(ax1, [], batch_results, param_dict.get('subtractD', False))\n",
    "            ax1.set_title('Number of Interactions by Type', fontsize=12)\n",
    "            \n",
    "            # Coefficient percentages\n",
    "            plot_coefficient_percentages(ax2, [], batch_results)\n",
    "            ax2.set_title('Coefficient Magnitude Distribution', fontsize=12)\n",
    "            \n",
    "            # Expected ratio\n",
    "            plot_interaction_counts_expected(ax3, [], batch_results)\n",
    "            ax3.set_title('Expected ones only', fontsize=12)\n",
    "            \n",
    "            # Total magnitude\n",
    "            plot_total_magnitude(ax4, [], batch_results)\n",
    "            ax4.set_title('Total Coefficient Magnitude', fontsize=12)\n",
    "            \n",
    "            fig_batch.suptitle(f'Batch Approach: {\", \".join(title_parts)}', fontsize=14, fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            if save_plots:\n",
    "                filename = f'sabra_batch_grouped_{param_str}.png'\n",
    "                fig_batch.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "                print(f\"Saved grouped batch figure: {filename}\")\n",
    "            \n",
    "            plt.close(fig_batch)\n",
    "        \n",
    "        # Create single approach figure if single results exist\n",
    "        if single_results:\n",
    "            fig_single, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 10))\n",
    "            \n",
    "            # Interaction counts\n",
    "            plot_interaction_counts(ax1, single_results, [], param_dict.get('subtractD', False))\n",
    "            ax1.set_title('Number of Interactions by Type', fontsize=12)\n",
    "            \n",
    "            # Coefficient percentages\n",
    "            plot_coefficient_percentages(ax2, single_results, [])\n",
    "            ax2.set_title('Coefficient Magnitude Distribution', fontsize=12)\n",
    "            \n",
    "            # Expected ratio\n",
    "            plot_interaction_counts_expected(ax3, [], single_results)\n",
    "            ax3.set_title('Number of expected ones only', fontsize=12)\n",
    "            \n",
    "            # Total magnitude\n",
    "            plot_total_magnitude(ax4, single_results, [])\n",
    "            ax4.set_title('Total Coefficient Magnitude', fontsize=12)\n",
    "            \n",
    "            fig_single.suptitle(f'Single Approach: {\", \".join(title_parts)}', fontsize=14, fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            if save_plots:\n",
    "                filename = f'sabra_single_grouped_{param_str}.png'\n",
    "                fig_single.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "                print(f\"Saved grouped single figure: {filename}\")\n",
    "            \n",
    "            plt.close(fig_single)\n",
    "    \n",
    "    plt.ion()  # Turn interactive mode back on\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14f31a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_sabra_approaches('/home/vale/SABRA/params_bin2/Ws_CV', nn=20, \n",
    "                        filter_params={'randombatch': True, 'knorm': False, 'subtractD': True},)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef538da",
   "metadata": {},
   "source": [
    "## assembling all the interactions from each shell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "263ac8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_top_interactions_per_shell(W, n_top=50, threshold=1e-10):\n",
    "    \"\"\"\n",
    "    Get top n interactions from a coefficient array W.\n",
    "    \n",
    "    Args:\n",
    "        W: Coefficient array\n",
    "        n_top: Number of top interactions to keep\n",
    "        threshold: Minimum coefficient magnitude to consider\n",
    "    \n",
    "    Returns:\n",
    "        dict: {feature_idx: coefficient_value} for top interactions\n",
    "    \"\"\"\n",
    "    # Handle different W formats\n",
    "    if isinstance(W, (list, tuple)):\n",
    "        if len(W) > 0:\n",
    "            W_array = np.array(W[0]) if hasattr(W[0], '__len__') else np.array(W)\n",
    "        else:\n",
    "            return {}\n",
    "    else:\n",
    "        W_array = np.array(W)\n",
    "    \n",
    "    # Flatten if multidimensional\n",
    "    if W_array.ndim > 1:\n",
    "        W_array = W_array.flatten()\n",
    "    \n",
    "    # Get non-zero coefficients with their indices\n",
    "    significant_coeffs = []\n",
    "    for idx, coeff in enumerate(W_array):\n",
    "        # Handle scalar vs array coefficients\n",
    "        if hasattr(coeff, '__len__') and len(coeff) > 1:\n",
    "            coeff_val = np.linalg.norm(coeff)\n",
    "        else:\n",
    "            coeff_val = float(coeff)\n",
    "            \n",
    "        if abs(coeff_val) >= threshold:\n",
    "            significant_coeffs.append((idx, coeff_val))\n",
    "    \n",
    "    # Sort by absolute coefficient value (descending)\n",
    "    significant_coeffs.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "    \n",
    "    # Take top n\n",
    "    top_coeffs = significant_coeffs[:n_top]\n",
    "    \n",
    "    return {idx: coeff for idx, coeff in top_coeffs}\n",
    "\n",
    "def analyze_interactions(W, nn, use_all_interactions=False, threshold=1e-5, expected_interactions=None, \n",
    "                        shell_idx=None, include_dissipation=False):\n",
    "    \"\"\"Analyze significant interactions in the learned model.\n",
    "    \n",
    "    Args:\n",
    "        W: array[n_shells, n_features] or array[1, n_features] - Learned coefficients\n",
    "        nn: int - Number of shells\n",
    "        use_all_interactions: bool\n",
    "        threshold: float\n",
    "        expected_interactions: dict\n",
    "        shell_idx: int or None - The shell being analyzed (0-based) or None for all shells\n",
    "        include_dissipation: bool - Whether dissipation terms are included in the dictionary\n",
    "    \"\"\"\n",
    "    significant = []\n",
    "    types = ['regular', 'j_conj', 'i_conj', 'both_conj'] if use_all_interactions else ['regular']\n",
    "    \n",
    "    # Convert to 2D array if needed\n",
    "    Wmat = W if W.ndim == 2 else W.reshape(1, -1)\n",
    "    n_shells_learned = Wmat.shape[0]\n",
    "    \n",
    "    # Determine which shells to analyze\n",
    "    if shell_idx is not None:\n",
    "        target_shells = [shell_idx]\n",
    "        shell_indices = [0]  # Index in Wmat\n",
    "    else:\n",
    "        target_shells = list(range(min(n_shells_learned, nn)))\n",
    "        shell_indices = list(range(n_shells_learned))\n",
    "    \n",
    "    for shell_w_idx, target_shell in zip(shell_indices, target_shells):\n",
    "        # Analyze interaction terms\n",
    "        for i in range(nn):\n",
    "            for j in range(nn):\n",
    "                for t, type_name in enumerate(types):\n",
    "                    idx = (i * nn + j) * (4 if use_all_interactions else 1) + t\n",
    "                    if idx >= Wmat.shape[1]:\n",
    "                        continue\n",
    "                        \n",
    "                    weight = Wmat[shell_w_idx, idx]\n",
    "                    if abs(weight) <= threshold:\n",
    "                        continue\n",
    "                    \n",
    "                    # Check if interaction is expected\n",
    "                    key = (i, j, type_name)\n",
    "                    expected_set = expected_interactions.get(target_shell, set()) if expected_interactions else set()\n",
    "                    status = 'expected' if key in expected_set else 'unexpected'\n",
    "                    \n",
    "                    significant.append({\n",
    "                        'target': target_shell + 1,  # Convert to 1-based indexing\n",
    "                        'i': i + 1,\n",
    "                        'j': j + 1,\n",
    "                        'type': type_name,\n",
    "                        'weight': weight,\n",
    "                        'highlight': status == 'expected',\n",
    "                        'status': status,\n",
    "                        'term_type': 'interaction'\n",
    "                    })\n",
    "        \n",
    "        # Analyze dissipation terms if included\n",
    "        if include_dissipation:\n",
    "            dissipation_start_idx = nn * nn * (4 if use_all_interactions else 1)\n",
    "            for i in range(nn):\n",
    "                idx = dissipation_start_idx + i\n",
    "                if idx >= Wmat.shape[1]:\n",
    "                    continue\n",
    "                    \n",
    "                weight = Wmat[shell_w_idx, idx]\n",
    "                if abs(weight) <= threshold:\n",
    "                    continue\n",
    "                \n",
    "                significant.append({\n",
    "                    'target': target_shell + 1,\n",
    "                    'i': i + 1,\n",
    "                    'j': i + 1,  # Self-interaction\n",
    "                    'type': 'dissipation',\n",
    "                    'weight': weight,\n",
    "                    'highlight': (i == target_shell),  # Highlight if dissipation comes from the target shell\n",
    "                    'status': 'expected' if (i == target_shell) else 'unexpected',\n",
    "                    'term_type': 'dissipation'\n",
    "                })\n",
    "    \n",
    "    return significant\n",
    "\n",
    "\n",
    "def assemble_final_W_from_shells(data_directory, nn=20, n_top_per_shell=50, \n",
    "                                filter_params=None, combination_method='average'):\n",
    "    \"\"\"\n",
    "    Assemble final W by collecting top interactions from each shell separately.\n",
    "    \n",
    "    Args:\n",
    "        data_directory: Path to directory containing W files\n",
    "        nn: Total number of shells\n",
    "        n_top_per_shell: Number of top interactions to keep from each shell\n",
    "        filter_params: Dict of parameter filters (excluding shell)\n",
    "        combination_method: How to combine coefficients ('average', 'max', 'median')\n",
    "    \n",
    "    Returns:\n",
    "        dict: {param_combo: (final_W, interaction_origins, feature_mapping)}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get all W files\n",
    "    files = [f for f in os.listdir(data_directory) if f.startswith('W_final') or f.startswith('W_list')]\n",
    "    \n",
    "    # Group files by parameter combination (excluding shell)\n",
    "    param_groups = defaultdict(list)\n",
    "    \n",
    "    for filename in files:\n",
    "        params = parse_filename(filename)\n",
    "        \n",
    "        # Apply filters if specified\n",
    "        if filter_params:\n",
    "            skip = False\n",
    "            for key, value in filter_params.items():\n",
    "                if key in params and params[key] != value:\n",
    "                    skip = True\n",
    "                    break\n",
    "            if skip:\n",
    "                continue\n",
    "        \n",
    "        # Group by all parameters except shell\n",
    "        param_key = tuple(sorted([(k, v) for k, v in params.items() \n",
    "                                if k not in ['shell', 'tstart', 'tend']]))\n",
    "        \n",
    "        param_groups[param_key].append((filename, params))\n",
    "    \n",
    "    # Process each parameter combination\n",
    "    results = {}\n",
    "    \n",
    "    for param_combo, file_list in param_groups.items():\n",
    "        print(f\"\\nProcessing parameter combination: {dict(param_combo)}\")\n",
    "        \n",
    "        # Get representative parameters for building feature mapping\n",
    "        rep_params = dict(param_combo)\n",
    "        include_dissipation = not rep_params.get('subtractD', False)\n",
    "        use_all_interactions = rep_params.get('allint', True)\n",
    "        \n",
    "        # Build feature mapping\n",
    "        feature_mapping = build_feature_mapping(nn, \n",
    "                                              use_all_interactions=use_all_interactions,\n",
    "                                              include_dissipation=include_dissipation)\n",
    "        \n",
    "        # Determine total features\n",
    "        max_feature_idx = max(feature_mapping.keys()) if feature_mapping else 0\n",
    "        total_features = max_feature_idx + 1\n",
    "        \n",
    "        # Collect top interactions from each shell separately\n",
    "        all_interactions = {}  # {feature_idx: [coeff_values]}\n",
    "        interaction_origins = {}  # {feature_idx: [(shell, approach)]}\n",
    "        \n",
    "        # Process each shell\n",
    "        shells_found = set()\n",
    "        for filename, params in file_list:\n",
    "            shells_found.add(params['shell'])\n",
    "        \n",
    "        print(f\"Found shells: {sorted(shells_found)}\")\n",
    "        \n",
    "        for shell_num in sorted(shells_found):\n",
    "            # Find file for this shell\n",
    "            shell_file = None\n",
    "            shell_params = None\n",
    "            for filename, params in file_list:\n",
    "                if params['shell'] == shell_num:\n",
    "                    shell_file = filename\n",
    "                    shell_params = params\n",
    "                    break\n",
    "            \n",
    "            if shell_file is None:\n",
    "                continue\n",
    "            \n",
    "            print(f\"  Processing shell {shell_num} from {shell_file}\")\n",
    "            \n",
    "            # Load coefficient data\n",
    "            filepath = os.path.join(data_directory, shell_file)\n",
    "            W_data = load_coefficient_data(filepath)\n",
    "            \n",
    "            if W_data is None:\n",
    "                continue\n",
    "            \n",
    "            # Handle different W data formats\n",
    "            if hasattr(W_data, 'shape') and len(W_data.shape) > 1 and W_data.shape[0] > 1:\n",
    "                # For single approach, use the last W (highest sample size)\n",
    "                W = W_data[-1] if shell_params['approach'] == 'single' else W_data[0]\n",
    "            else:\n",
    "                W = W_data\n",
    "            \n",
    "            # Get top interactions for this shell\n",
    "            top_interactions = get_top_interactions_per_shell(W, n_top_per_shell)\n",
    "            \n",
    "            print(f\"    Found {len(top_interactions)} significant interactions\")\n",
    "            \n",
    "            # Store interactions with their shell origin\n",
    "            for feature_idx, coeff_val in top_interactions.items():\n",
    "                if feature_idx < total_features:  # Safety check\n",
    "                    if feature_idx not in all_interactions:\n",
    "                        all_interactions[feature_idx] = []\n",
    "                        interaction_origins[feature_idx] = []\n",
    "                    \n",
    "                    all_interactions[feature_idx].append(coeff_val)\n",
    "                    interaction_origins[feature_idx].append((shell_num, shell_params['approach']))\n",
    "        \n",
    "        # Combine coefficients using specified method\n",
    "        final_W = np.zeros(total_features)\n",
    "        \n",
    "        for feature_idx, coeff_values in all_interactions.items():\n",
    "            if combination_method == 'average':\n",
    "                final_W[feature_idx] = np.mean(coeff_values)\n",
    "            elif combination_method == 'max':\n",
    "                # Take coefficient with maximum absolute value\n",
    "                max_idx = np.argmax([abs(c) for c in coeff_values])\n",
    "                final_W[feature_idx] = coeff_values[max_idx]\n",
    "            elif combination_method == 'median':\n",
    "                final_W[feature_idx] = np.median(coeff_values)\n",
    "            else:\n",
    "                final_W[feature_idx] = np.mean(coeff_values)  # Default to average\n",
    "        \n",
    "        print(f\"  Final W has {np.sum(np.abs(final_W) > 1e-10)} non-zero coefficients\")\n",
    "        \n",
    "        # Store results for this parameter combination\n",
    "        results[param_combo] = (final_W, interaction_origins, feature_mapping)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def create_interaction_origin_summary_dict(interaction_origins, feature_mapping, nn):\n",
    "    \"\"\"\n",
    "    Create a summary of interaction origins for analysis.\n",
    "    \n",
    "    Args:\n",
    "        interaction_origins: Dict mapping feature indices to origin info\n",
    "        feature_mapping: Dict mapping feature indices to interactions\n",
    "        nn: Number of shells\n",
    "    \n",
    "    Returns:\n",
    "        dict: Summary of origins by shell and interaction type\n",
    "    \"\"\"\n",
    "    origin_summary = {\n",
    "        'by_shell': defaultdict(int),\n",
    "        'by_approach': defaultdict(int),\n",
    "        'by_interaction_type': defaultdict(int),\n",
    "        'shell_contributions': defaultdict(lambda: defaultdict(int)),\n",
    "        'detailed_interactions': []\n",
    "    }\n",
    "    \n",
    "    for feature_idx, origins in interaction_origins.items():\n",
    "        if feature_idx in feature_mapping:\n",
    "            interaction = feature_mapping[feature_idx]\n",
    "            \n",
    "            # Determine interaction type\n",
    "            if len(interaction) == 2 and interaction[1] == 'dissipation':\n",
    "                int_type = 'dissipation'\n",
    "                interaction_str = f\"D_{interaction[0]}\"\n",
    "            else:\n",
    "                int_type = 'interaction'\n",
    "                interaction_str = f\"{interaction[0]}+{interaction[1]}{interaction[0]+interaction[1]}\"\n",
    "            \n",
    "            # Record detailed interaction info\n",
    "            origin_summary['detailed_interactions'].append({\n",
    "                'feature_idx': feature_idx,\n",
    "                'interaction': interaction,\n",
    "                'interaction_str': interaction_str,\n",
    "                'type': int_type,\n",
    "                'origins': origins\n",
    "            })\n",
    "            \n",
    "            for shell, approach in origins:\n",
    "                origin_summary['by_shell'][shell] += 1\n",
    "                origin_summary['by_approach'][approach] += 1\n",
    "                origin_summary['by_interaction_type'][int_type] += 1\n",
    "                origin_summary['shell_contributions'][shell][int_type] += 1\n",
    "    \n",
    "    return origin_summary\n",
    "\n",
    "\n",
    "def save_final_W_and_origins(final_W, interaction_origins, feature_mapping, \n",
    "                           output_directory, filename_prefix=\"final_W\"):\n",
    "    \"\"\"\n",
    "    Save the final W and origin information to files.\n",
    "    \n",
    "    Args:\n",
    "        final_W: Final coefficient array\n",
    "        interaction_origins: Origin information\n",
    "        feature_mapping: Feature mapping\n",
    "        output_directory: Where to save files\n",
    "        filename_prefix: Prefix for output files\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    \n",
    "    # Save final W\n",
    "    np.save(os.path.join(output_directory, f\"{filename_prefix}.npy\"), final_W)\n",
    "    \n",
    "    # Save origins and mapping\n",
    "    with open(os.path.join(output_directory, f\"{filename_prefix}_origins.pkl\"), 'wb') as f:\n",
    "        pickle.dump(interaction_origins, f)\n",
    "    \n",
    "    with open(os.path.join(output_directory, f\"{filename_prefix}_mapping.pkl\"), 'wb') as f:\n",
    "        pickle.dump(feature_mapping, f)\n",
    "    \n",
    "    # Create human-readable summary\n",
    "    origin_summary = create_interaction_origin_summary(interaction_origins, feature_mapping, 20)\n",
    "    \n",
    "    with open(os.path.join(output_directory, f\"{filename_prefix}_summary.txt\"), 'w') as f:\n",
    "        f.write(\"FINAL W INTERACTION ORIGINS SUMMARY\\n\")\n",
    "        f.write(\"=\" * 40 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"Contributions by Shell:\\n\")\n",
    "        for shell in sorted(origin_summary['by_shell'].keys()):\n",
    "            f.write(f\"  Shell {shell}: {origin_summary['by_shell'][shell]} interactions\\n\")\n",
    "        \n",
    "        f.write(f\"\\nContributions by Approach:\\n\")\n",
    "        for approach, count in origin_summary['by_approach'].items():\n",
    "            f.write(f\"  {approach}: {count} interactions\\n\")\n",
    "        \n",
    "        f.write(f\"\\nContributions by Interaction Type:\\n\")\n",
    "        for int_type, count in origin_summary['by_interaction_type'].items():\n",
    "            f.write(f\"  {int_type}: {count} interactions\\n\")\n",
    "        \n",
    "        f.write(f\"\\nDetailed Shell Contributions:\\n\")\n",
    "        for shell in sorted(origin_summary['shell_contributions'].keys()):\n",
    "            f.write(f\"  Shell {shell}:\\n\")\n",
    "            for int_type, count in origin_summary['shell_contributions'][shell].items():\n",
    "                f.write(f\"    {int_type}: {count} interactions\\n\")\n",
    "\n",
    "\n",
    "def plot_coupled_results2(W, expected_shells, nn=20, n_strongest=30, allint=True, \n",
    "                         subtractD=False, filename=None, interaction_origins=None, \n",
    "                         feature_mapping=None, show_origin_info=True):\n",
    "    \"\"\"\n",
    "    Plot SABRA regression results with optional interaction origin visualization.\n",
    "    Maintains backward compatibility with original plot structure.\n",
    "    \n",
    "    Args:\n",
    "        W: Coefficient array or final assembled W\n",
    "        expected_shells: List of shells to consider for expected interactions\n",
    "        nn: Number of shells\n",
    "        n_strongest: Number of strongest coefficients to highlight\n",
    "        allint: Whether all interactions were used\n",
    "        subtractD: Whether dissipation was subtracted\n",
    "        filename: Optional filename to save plot\n",
    "        interaction_origins: Dict mapping feature indices to origin info (optional)\n",
    "        feature_mapping: Dict mapping feature indices to interactions (optional)\n",
    "        show_origin_info: Whether to show origin information in plots\n",
    "    \"\"\"\n",
    "    \n",
    "    # Handle W format\n",
    "    if isinstance(W, (list, tuple)):\n",
    "        if len(W) > 0:\n",
    "            W_array = np.array(W[0]) if hasattr(W[0], '__len__') else np.array(W)\n",
    "        else:\n",
    "            print(\"Empty W provided\")\n",
    "            return\n",
    "    else:\n",
    "        W_array = np.array(W)\n",
    "    \n",
    "    if W_array.ndim > 1:\n",
    "        W_array = W_array.flatten()\n",
    "    \n",
    "    # Build feature mapping if not provided\n",
    "    if feature_mapping is None:\n",
    "        include_dissipation = not subtractD\n",
    "        feature_mapping = build_feature_mapping(nn, use_all_interactions=allint, \n",
    "                                              include_dissipation=include_dissipation)\n",
    "    \n",
    "    # Get expected interactions for all expected shells\n",
    "    all_expected_interactions = set()\n",
    "    for shell_idx in expected_shells:\n",
    "        shell_expected = get_expected_interactions_single_shell(nn, shell_idx - 1)  # Convert to 0-based\n",
    "        all_expected_interactions.update(shell_expected)\n",
    "    \n",
    "    # Analyze coefficients (original logic preserved)\n",
    "    expected_coeff_sum = 0\n",
    "    unexpected_coeff_sum = 0\n",
    "    expected_dissipation_sum = 0\n",
    "    unexpected_dissipation_sum = 0\n",
    "    expected_count = 0\n",
    "    unexpected_count = 0\n",
    "    expected_dissipation_count = 0\n",
    "    unexpected_dissipation_count = 0\n",
    "    \n",
    "    # Track coefficients for detailed plotting\n",
    "    expected_coeffs = []\n",
    "    unexpected_coeffs = []\n",
    "    expected_dissipation_coeffs = []\n",
    "    unexpected_dissipation_coeffs = []\n",
    "    origin_mismatch_info = {}  # Track origin mismatches\n",
    "    \n",
    "    # Find significant coefficients\n",
    "    significant_coeffs = []\n",
    "    for idx, coeff in enumerate(W_array):\n",
    "        if hasattr(coeff, '__len__') and len(coeff) > 1:\n",
    "            coeff_val = np.linalg.norm(coeff)\n",
    "        else:\n",
    "            coeff_val = float(coeff)\n",
    "            \n",
    "        if abs(coeff_val) > 1e-10:\n",
    "            significant_coeffs.append((idx, coeff_val))\n",
    "    \n",
    "    # Sort and get top coefficients\n",
    "    significant_coeffs.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "    top_coeffs = significant_coeffs[:n_strongest]\n",
    "    \n",
    "    # Categorize coefficients (original logic)\n",
    "    for idx, coeff_val in significant_coeffs:\n",
    "        if idx in feature_mapping:\n",
    "            interaction = feature_mapping[idx]\n",
    "            \n",
    "            # Check for origin mismatch if we have origin info\n",
    "            has_origin_mismatch = False\n",
    "            mismatch_details = []\n",
    "            if interaction_origins and idx in interaction_origins and show_origin_info:\n",
    "                if len(interaction) == 3:  # Regular interaction (not dissipation)\n",
    "                    i, j, int_type = interaction\n",
    "                    is_globally_expected = (i, j, int_type) in all_expected_interactions\n",
    "                    \n",
    "                    if is_globally_expected:\n",
    "                        for origin_shell,approach in interaction_origins[idx]:\n",
    "                            shell_expected = get_expected_interactions_single_shell(nn, origin_shell - 1)\n",
    "                            if (i, j, int_type) not in shell_expected:\n",
    "                                has_origin_mismatch = True\n",
    "                                mismatch_details.append((origin_shell, approach))\n",
    "            \n",
    "            if len(interaction) == 2 and interaction[1] == 'dissipation':\n",
    "                # Dissipation term\n",
    "                dissipation_shell = interaction[0]\n",
    "                if dissipation_shell in [s-1 for s in expected_shells]:  # Convert to 0-based\n",
    "                    expected_dissipation_sum += abs(coeff_val)\n",
    "                    expected_dissipation_count += 1\n",
    "                    expected_dissipation_coeffs.append((idx, coeff_val, interaction))\n",
    "                else:\n",
    "                    unexpected_dissipation_sum += abs(coeff_val)\n",
    "                    unexpected_dissipation_count += 1\n",
    "                    unexpected_dissipation_coeffs.append((idx, coeff_val, interaction))\n",
    "            else:\n",
    "                # Regular interaction\n",
    "                i, j, int_type = interaction\n",
    "                if (i, j, int_type) in all_expected_interactions:\n",
    "                    expected_coeff_sum += abs(coeff_val)\n",
    "                    expected_count += 1\n",
    "                    expected_coeffs.append((idx, coeff_val, interaction))\n",
    "                    \n",
    "                    # Track origin mismatch for expected interactions\n",
    "                    if has_origin_mismatch:\n",
    "                        origin_mismatch_info[idx] = mismatch_details\n",
    "                else:\n",
    "                    unexpected_coeff_sum += abs(coeff_val)\n",
    "                    unexpected_count += 1\n",
    "                    unexpected_coeffs.append((idx, coeff_val, interaction))\n",
    "    \n",
    "    total_sum = expected_coeff_sum + unexpected_coeff_sum + expected_dissipation_sum + unexpected_dissipation_sum\n",
    "    \n",
    "    # Create the original plot structure\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # Main plot: Coefficient values (original)\n",
    "    ax1 = plt.subplot(2, 3, (1, 2))\n",
    "    \n",
    "    # Plot top coefficients with original color scheme\n",
    "    indices = list(range(len(top_coeffs)))\n",
    "    coeff_values = [coeff for _, coeff in top_coeffs]\n",
    "    \n",
    "    # Color by category (original logic)\n",
    "    colors = []\n",
    "    markers = []\n",
    "    for idx, coeff_val in top_coeffs:\n",
    "        if idx in feature_mapping:\n",
    "            interaction = feature_mapping[idx]\n",
    "            \n",
    "            if len(interaction) == 2 and interaction[1] == 'dissipation':\n",
    "                dissipation_shell = interaction[0]\n",
    "                if dissipation_shell in [s-1 for s in expected_shells]:\n",
    "                    colors.append('blue')  # Expected dissipation\n",
    "                else:\n",
    "                    colors.append('lightblue')  # Unexpected dissipation\n",
    "                markers.append('s')  # Square for dissipation\n",
    "            else:\n",
    "                i, j, int_type = interaction\n",
    "                if (i, j, int_type) in all_expected_interactions:\n",
    "                    # Check for origin mismatch\n",
    "                    if idx in origin_mismatch_info and show_origin_info:\n",
    "                        colors.append('orange')  # Expected but wrong origin\n",
    "                        markers.append('^')  # Triangle for mismatch\n",
    "                    else:\n",
    "                        colors.append('green')  # Expected\n",
    "                        markers.append('o')  # Circle for expected\n",
    "                else:\n",
    "                    colors.append('red')  # Unexpected\n",
    "                    markers.append('o')  # Circle for unexpected\n",
    "        else:\n",
    "            colors.append('gray')\n",
    "            markers.append('o')\n",
    "    \n",
    "    # Create scatter plot (preserving original style)\n",
    "    for i, (idx_val, coeff_val) in enumerate(top_coeffs):\n",
    "        ax1.scatter(i, coeff_val, c=colors[i], marker=markers[i], s=60, alpha=0.7)\n",
    "    \n",
    "    ax1.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    ax1.set_xlabel('Coefficient Index (ranked by magnitude)')\n",
    "    ax1.set_ylabel('Coefficient Value')\n",
    "    ax1.set_title(f'Top {n_strongest} Strongest Coefficients')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Original legend with origin info addition\n",
    "    legend_elements = [\n",
    "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='green', \n",
    "                   markersize=8, label='Expected', alpha=0.7),\n",
    "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='red', \n",
    "                   markersize=8, label='Unexpected', alpha=0.7),\n",
    "        plt.Line2D([0], [0], marker='s', color='w', markerfacecolor='blue', \n",
    "                   markersize=8, label='Expected Dissipation', alpha=0.7),\n",
    "        plt.Line2D([0], [0], marker='s', color='w', markerfacecolor='lightblue', \n",
    "                   markersize=8, label='Unexpected Dissipation', alpha=0.7)\n",
    "    ]\n",
    "    \n",
    "    if origin_mismatch_info and show_origin_info:\n",
    "        legend_elements.append(\n",
    "            plt.Line2D([0], [0], marker='^', color='w', markerfacecolor='orange', \n",
    "                       markersize=8, label='Origin Mismatch', alpha=0.7)\n",
    "        )\n",
    "    \n",
    "    ax1.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    # Summary pie chart (original)\n",
    "    ax2 = plt.subplot(2, 3, 3)\n",
    "    \n",
    "    pie_data = []\n",
    "    pie_labels = []\n",
    "    pie_colors = []\n",
    "    \n",
    "    if expected_coeff_sum > 0:\n",
    "        pie_data.append(expected_coeff_sum)\n",
    "        pie_labels.append(f'Expected\\n({expected_count})')\n",
    "        pie_colors.append('green')\n",
    "    \n",
    "    if unexpected_coeff_sum > 0:\n",
    "        pie_data.append(unexpected_coeff_sum)\n",
    "        pie_labels.append(f'Unexpected\\n({unexpected_count})')\n",
    "        pie_colors.append('red')\n",
    "    \n",
    "    if expected_dissipation_sum > 0:\n",
    "        pie_data.append(expected_dissipation_sum)\n",
    "        pie_labels.append(f'Exp. Dissip.\\n({expected_dissipation_count})')\n",
    "        pie_colors.append('blue')\n",
    "    \n",
    "    if unexpected_dissipation_sum > 0:\n",
    "        pie_data.append(unexpected_dissipation_sum)\n",
    "        pie_labels.append(f'Unexp. Dissip.\\n({unexpected_dissipation_count})')\n",
    "        pie_colors.append('lightblue')\n",
    "    \n",
    "    if pie_data:\n",
    "        ax2.pie(pie_data, labels=pie_labels, colors=pie_colors, autopct='%1.1f%%', startangle=90)\n",
    "        ax2.set_title('Coefficient Sum Distribution')\n",
    "    \n",
    "    # NEW: Origin analysis plots (only if origin info available)\n",
    "    if interaction_origins and show_origin_info:\n",
    "        # Origin shell distribution\n",
    "        ax3 = plt.subplot(2, 3, 4)\n",
    "        \n",
    "        origin_shell_counts = defaultdict(int)\n",
    "        origin_approach_counts = defaultdict(int)\n",
    "        \n",
    "        for idx, coeff_val in top_coeffs:\n",
    "            if idx in interaction_origins:\n",
    "                for origin_shell, approach in interaction_origins[idx]:\n",
    "                    origin_shell_counts[origin_shell] += 1\n",
    "                    origin_approach_counts[approach] += 1\n",
    "        \n",
    "        if origin_shell_counts:\n",
    "            shells = sorted(origin_shell_counts.keys())\n",
    "            counts = [origin_shell_counts[s] for s in shells]\n",
    "            \n",
    "            bars = ax3.bar(shells, counts, alpha=0.7, color='skyblue', edgecolor='navy')\n",
    "            ax3.set_xlabel('Origin Shell')\n",
    "            ax3.set_ylabel('Number of Top Interactions')\n",
    "            ax3.set_title('Interaction Origins by Shell')\n",
    "            ax3.set_xticks(shells)\n",
    "            \n",
    "            # Highlight shells with mismatches\n",
    "            if origin_mismatch_info:\n",
    "                mismatch_shells = set()\n",
    "                for mismatch_details in origin_mismatch_info.values():\n",
    "                    for shell, _ in mismatch_details:\n",
    "                        mismatch_shells.add(shell)\n",
    "                \n",
    "                for i, shell in enumerate(shells):\n",
    "                    if shell in mismatch_shells:\n",
    "                        bars[i].set_color('orange')\n",
    "                        bars[i].set_alpha(0.8)\n",
    "        \n",
    "        # Approach distribution\n",
    "        ax4 = plt.subplot(2, 3, 5)\n",
    "        \n",
    "        if origin_approach_counts:\n",
    "            approaches = list(origin_approach_counts.keys())\n",
    "            counts = list(origin_approach_counts.values())\n",
    "            \n",
    "            ax4.pie(counts, labels=approaches, autopct='%1.1f%%', startangle=90)\n",
    "            ax4.set_title('Interactions by Approach')\n",
    "        \n",
    "        # Origin mismatch details\n",
    "        ax5 = plt.subplot(2, 3, 6)\n",
    "        \n",
    "        if origin_mismatch_info:\n",
    "            mismatch_text = \"Origin Mismatches:\\n\\n\"\n",
    "            for idx, mismatch_details in list(origin_mismatch_info.items())[:10]:  # Show first 10\n",
    "                if idx in feature_mapping:\n",
    "                    interaction = feature_mapping[idx]\n",
    "                    mismatch_text += f\"Interaction {interaction}:\\n\"\n",
    "                    for shell, approach in mismatch_details:\n",
    "                        mismatch_text += f\"  From shell {shell} ({approach})\\n\"\n",
    "                    mismatch_text += \"\\n\"\n",
    "            \n",
    "            ax5.text(0.05, 0.95, mismatch_text, transform=ax5.transAxes, \n",
    "                    verticalalignment='top', fontsize=8, fontfamily='monospace')\n",
    "            ax5.set_xlim(0, 1)\n",
    "            ax5.set_ylim(0, 1)\n",
    "            ax5.axis('off')\n",
    "            ax5.set_title('Origin Mismatch Details')\n",
    "        else:\n",
    "            ax5.text(0.5, 0.5, 'No origin mismatches detected', \n",
    "                    transform=ax5.transAxes, ha='center', va='center')\n",
    "            ax5.axis('off')\n",
    "            ax5.set_title('Origin Analysis')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Print original summary with origin info\n",
    "    print(f\"\\nOriginal Analysis Summary:\")\n",
    "    print(f\"Expected interactions: {expected_count} (sum: {expected_coeff_sum:.4f})\")\n",
    "    print(f\"Unexpected interactions: {unexpected_count} (sum: {unexpected_coeff_sum:.4f})\")\n",
    "    print(f\"Expected dissipation: {expected_dissipation_count} (sum: {expected_dissipation_sum:.4f})\")\n",
    "    print(f\"Unexpected dissipation: {unexpected_dissipation_count} (sum: {unexpected_dissipation_sum:.4f})\")\n",
    "    print(f\"Total sum: {total_sum:.4f}\")\n",
    "    \n",
    "    if expected_coeff_sum + unexpected_coeff_sum > 0:\n",
    "        print(f\"Expected percentage: {expected_coeff_sum/(expected_coeff_sum + unexpected_coeff_sum)*100:.1f}%\")\n",
    "    \n",
    "    if origin_mismatch_info and show_origin_info:\n",
    "        print(f\"\\nOrigin Analysis:\")\n",
    "        print(f\"Interactions with origin mismatches: {len(origin_mismatch_info)}\")\n",
    "        origin_shell_counts = defaultdict(int)\n",
    "        for origins in interaction_origins.values():\n",
    "            for shell, approach in origins:\n",
    "                origin_shell_counts[shell] += 1\n",
    "        print(f\"Origin shell distribution: {dict(origin_shell_counts)}\")\n",
    "    \n",
    "    if filename:\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nPlot saved as: {filename}\")\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    return {\n",
    "        'expected_percentage': (expected_coeff_sum/(expected_coeff_sum + unexpected_coeff_sum)*100) if (expected_coeff_sum + unexpected_coeff_sum > 0) else 0,\n",
    "        'expected_coeffs': expected_coeffs,\n",
    "        'unexpected_coeffs': unexpected_coeffs,\n",
    "        'expected_dissipation_coeffs': expected_dissipation_coeffs,\n",
    "        'unexpected_dissipation_coeffs': unexpected_dissipation_coeffs,\n",
    "        'origin_mismatch_info': origin_mismatch_info,\n",
    "        'top_coeffs': top_coeffs\n",
    "    }\n",
    "\n",
    "def get_expected_interactions(nn):\n",
    "    \"\"\"\n",
    "    Build a dict of expected (i, j, type) tuples for each shell index n (0-based).\n",
    "    Only the three model terms:\n",
    "      1) i k_{n+1} A u_{n+1}^* u_{n+2}     type 'i_conj'\n",
    "      2) i k_n     B u_{n-1}^* u_{n+1}     type 'i_conj'\n",
    "      3) -i k_{n-1} C u_{n-2}   u_{n-1}    type 'regular'\n",
    "    Also includes symmetry: (i, j, 'i_conj') is equivalent to (j, i, 'j_conj').\n",
    "    Also (i, j, 'regular') is equivalent to (j, i, 'regular').\n",
    "    \"\"\"\n",
    "    expected = {}\n",
    "    for n in range(nn):\n",
    "        terms = set()\n",
    "        # term 1 (forward-forward)\n",
    "        if n <= nn-3:\n",
    "            # (n+1, n+2, 'i_conj') and its symmetric (n+2, n+1, 'j_conj')\n",
    "            terms.add((n+1, n+2, 'i_conj'))\n",
    "            terms.add((n+2, n+1, 'j_conj'))\n",
    "        # term 2 (backward-forward)\n",
    "        if 1 <= n <= nn-2:\n",
    "            # (n-1, n+1, 'i_conj') and its symmetric (n+1, n-1, 'j_conj')\n",
    "            terms.add((n-1, n+1, 'i_conj'))\n",
    "            terms.add((n+1, n-1, 'j_conj'))\n",
    "        # term 3 (backward-backward)\n",
    "        if n >= 2:\n",
    "            # (n-2, n-1, 'regular') and its symmetric (n-1, n-2, 'regular')\n",
    "            terms.add((n-2, n-1, 'regular'))\n",
    "            terms.add((n-1, n-2, 'regular'))\n",
    "        expected[n] = terms\n",
    "    return expected\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b26985",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn=20\n",
    "data_dir = '/home/vale/SABRA/params_bin/Ws'\n",
    "expected_shells = list(range(nn)) # Example expected shells\n",
    "subtract_dissipation= False\n",
    "\n",
    "\n",
    "results = assemble_final_W_from_shells(\n",
    "    data_directory=data_dir,\n",
    "    nn=nn,\n",
    "    n_top_per_shell=7,\n",
    "    filter_params={ 'subtractD': subtract_dissipation} # example filters\n",
    ")\n",
    "\n",
    "# Then for each parameter combination:\n",
    "for param_combo, (final_W, interaction_origins, feature_mapping) in results.items():\n",
    "    filename='stats_Wfinal'\n",
    "    filename+=f\"_{'_'.join([f'{k}{v}' for k, v in param_combo])}.png\"\n",
    "\n",
    "    plot_coupled_results2(final_W, expected_shells=expected_shells, nn=nn,\n",
    "                         interaction_origins=interaction_origins,\n",
    "                         feature_mapping=feature_mapping,filename=filename,)\n",
    "    \n",
    "    plot_suffix= f\"_{'_'.join([f'{k}{v}' for k, v in param_combo])}\"\n",
    "    #plot_suffix+= 'final_W'\n",
    "\n",
    "    plot_coupled_results(final_W, expected_shells=expected_shells, nn=nn,top_n=90, \n",
    "                         include_dissipation=not subtract_dissipation,use_all_interactions=True,plot_suffix=plot_suffix,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673a013f",
   "metadata": {},
   "source": [
    "## Ranking interactions shell by shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e3cd6127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_expected_interactions_by_shell(nn, refined, t_start, normalize_by_k, subtract_dissipation,\n",
    "                                        use_lassoCV, lambda_reg, expected_interactions,\n",
    "                                        include_dissipation, nbatch=None, minocc=None, \n",
    "                                        save_dir='Ws', output_dir='rankings'):\n",
    "    \"\"\"\n",
    "    Load W_list files and plot expected interactions/dissipations shell-by-shell, ordered by magnitude.\n",
    "\n",
    "    Args:\n",
    "        nn (int): Number of shells.\n",
    "        refined (str): Refinement label in filename (can be empty).\n",
    "        t_start (float): Starting time index used in filenames.\n",
    "        normalize_by_k (bool): Whether normalization by k was used.\n",
    "        subtract_dissipation (bool): Whether dissipation terms were subtracted.\n",
    "        use_lassoCV (bool): Whether LassoCV was used.\n",
    "        lambda_reg (float): Regularization strength (ignored if LassoCV is True).\n",
    "        expected_interactions (dict): Dict of expected interactions per shell.\n",
    "        include_dissipation (bool): Whether dissipation terms are included in the model.\n",
    "        nbatch (int, optional): Number of batches parameter to filter files.\n",
    "        minocc (int, optional): Minimum occurrence parameter to filter files.\n",
    "        save_dir (str): Folder where W_list_*.npy files are stored.\n",
    "        output_dir (str): Folder where plot will be saved.\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_expected_ranking(shell_idx, nn):\n",
    "        \"\"\"Compute expected coefficient ranking for a given shell based on SABRA theory.\"\"\"\n",
    "        r = 2\n",
    "        A = 1\n",
    "        B = A/r - A  # -0.5\n",
    "        C = -A/r     # -0.5\n",
    "        nu = 1e-8\n",
    "        \n",
    "        # Compute wavenumbers (0-based indexing)\n",
    "        k = r ** np.arange(nn)\n",
    "        \n",
    "        # Store expected coefficients with their identifiers\n",
    "        expected_coeffs = []\n",
    "        \n",
    "        # Shell indexing: shell_idx is 0-based, but SABRA equations use 1-based (n)\n",
    "        n = shell_idx + 1\n",
    "        \n",
    "        # Add interaction terms based on SABRA equations\n",
    "        # Note: k array is 0-based, but SABRA equations use 1-based indexing\n",
    "        if n == 1:  # First shell\n",
    "            if n+1 < nn:  # Check bounds for u_3\n",
    "                coeff_mag = k[n] * abs(A)  # k_2 * A for u_2^* u_3 (k[1] in 0-based)\n",
    "                expected_coeffs.append((coeff_mag, ('interaction', n+1, n+2, 'i_conj')))\n",
    "                \n",
    "        elif n == 2:  # Second shell\n",
    "            if n+1 < nn:  # k_3 A u_3^* u_4 term\n",
    "                coeff_mag = k[n] * abs(A)  # k[2] for k_3\n",
    "                expected_coeffs.append((coeff_mag, ('interaction', n+1, n+2, 'i_conj')))\n",
    "            # k_2 B u_1^* u_3 term\n",
    "            coeff_mag = k[n-1] * abs(B)  # k[1] for k_2\n",
    "            expected_coeffs.append((coeff_mag, ('interaction', n-1, n+1, 'i_conj')))\n",
    "            \n",
    "        elif n == nn-1:  # Penultimate shell\n",
    "            # k_{nn-1} B u_{nn-2}^* u_{nn} term\n",
    "            coeff_mag = k[n-1] * abs(B)  # k[n-2] for k_{nn-1}\n",
    "            expected_coeffs.append((coeff_mag, ('interaction', n-1, n+1, 'i_conj')))\n",
    "            # k_{nn-2} C u_{nn-3} u_{nn-2} term\n",
    "            if n-2 >= 1:\n",
    "                coeff_mag = k[n-3] * abs(C)  # k[n-4] for k_{nn-2}\n",
    "                expected_coeffs.append((coeff_mag, ('interaction', n-2, n-1, 'regular')))\n",
    "                \n",
    "        elif n == nn:  # Last shell\n",
    "            # k_{nn-1} C u_{nn-2} u_{nn-1} term\n",
    "            coeff_mag = k[n-2] * abs(C)  # k[n-3] for k_{nn-1}\n",
    "            expected_coeffs.append((coeff_mag, ('interaction', n-1, n, 'regular')))\n",
    "            \n",
    "        else:  # Middle shells (3 to nn-2)\n",
    "            if n+1 < nn:  # Forward term: k_{n+1} A u_{n+1}^* u_{n+2}\n",
    "                coeff_mag = k[n] * abs(A)  # k[n-1] for k_{n+1}\n",
    "                expected_coeffs.append((coeff_mag, ('interaction', n+1, n+2, 'i_conj')))\n",
    "            # Middle term: k_n B u_{n-1}^* u_{n+1}\n",
    "            coeff_mag = k[n-1] * abs(B)  # k[n-2] for k_n\n",
    "            expected_coeffs.append((coeff_mag, ('interaction', n-1, n+1, 'i_conj')))\n",
    "            # Backward term: k_{n-1} C u_{n-2} u_{n-1}\n",
    "            if n-2 >= 1:\n",
    "                coeff_mag = k[n-2] * abs(C)  # k[n-3] for k_{n-1}\n",
    "                expected_coeffs.append((coeff_mag, ('interaction', n-1, n, 'regular')))\n",
    "        \n",
    "        # Add dissipation term if included\n",
    "        if include_dissipation:\n",
    "            coeff_mag = nu * k[n-1]**2  # k[n-2] for k_n\n",
    "            expected_coeffs.append((coeff_mag, ('dissipation', n)))\n",
    "        \n",
    "        # Sort by magnitude (descending)\n",
    "        expected_coeffs.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        # Return just the identifiers in ranked order\n",
    "        return [coeff[1] for coeff in expected_coeffs]\n",
    "\n",
    "    type_colors = {\n",
    "        'forward-forward': 'green',\n",
    "        'backward-backward': 'orange',\n",
    "        'backward-forward': 'blue',\n",
    "        'dissipation': 'red'\n",
    "    }\n",
    "\n",
    "    # Adjust regex to match both old and new filename formats\n",
    "    # Old format: W_list__tstart...\n",
    "    # New format: W_final_shell1_nbatch5_tstart1_tend50000_allintTrue_iTrue_subtractDFalse_knormFalse_lassoCVTrue_lambda500.0_randombatchFalse_sigthresh100.0_minocc5\n",
    "    \n",
    "    shell_data = {}\n",
    "    max_shell = 0\n",
    "\n",
    "    for fname in os.listdir(save_dir):\n",
    "        if fname.startswith(\"W_\") and fname.endswith(\".npy\"):\n",
    "            shell_idx = None\n",
    "            \n",
    "            # Try new format first\n",
    "            new_pattern = rf\"W_final_shell(\\d+)_nbatch(\\d+)_tstart{t_start}_.*_subtractD{subtract_dissipation}_knorm{normalize_by_k}_lassoCVTrue.*_minocc(\\d+)\"\n",
    "            if use_lassoCV:\n",
    "                match = re.search(new_pattern, fname)\n",
    "                if match:\n",
    "                    file_shell_idx = int(match.group(1)) - 1  # 0-based shell index\n",
    "                    file_nbatch = int(match.group(2))\n",
    "                    file_minocc = int(match.group(3))\n",
    "                    \n",
    "                    # Check if parameters match (if specified)\n",
    "                    if (nbatch is None or file_nbatch == nbatch) and (minocc is None or file_minocc == minocc):\n",
    "                        shell_idx = file_shell_idx\n",
    "            else:\n",
    "                # For non-lassoCV, adjust pattern to include lambda value\n",
    "                new_pattern_lambda = rf\"W_final_shell(\\d+)_nbatch(\\d+)_tstart{t_start}_.*_subtractD{subtract_dissipation}_knorm{normalize_by_k}_lassoCVFalse_lambda{lambda_reg}_.*_minocc(\\d+)\"\n",
    "                match = re.search(new_pattern_lambda, fname)\n",
    "                if match:\n",
    "                    file_shell_idx = int(match.group(1)) - 1  # 0-based shell index\n",
    "                    file_nbatch = int(match.group(2))\n",
    "                    file_minocc = int(match.group(3))\n",
    "                    \n",
    "                    # Check if parameters match (if specified)\n",
    "                    if (nbatch is None or file_nbatch == nbatch) and (minocc is None or file_minocc == minocc):\n",
    "                        shell_idx = file_shell_idx\n",
    "            \n",
    "            # If new format didn't match, try old format\n",
    "            if shell_idx is None:\n",
    "                old_pattern = (f\"_tstart{t_start}_shell(\\\\d+)_nn{nn}_\"\n",
    "                              f\"lambda{(lambda_reg if not use_lassoCV else '')}_\"\n",
    "                              f\"lassoCV{use_lassoCV}_subtractD{subtract_dissipation}_knorm{normalize_by_k}\")\n",
    "                match = re.search(old_pattern, fname)\n",
    "                if match:\n",
    "                    shell_idx = int(match.group(1)) - 1  # 0-based shell index\n",
    "            \n",
    "            if shell_idx is not None:\n",
    "                full_path = os.path.join(save_dir, fname)\n",
    "                W = np.load(full_path, allow_pickle=True)\n",
    "                W = W[()] if isinstance(W, np.ndarray) and W.shape == () else W\n",
    "\n",
    "                significant = analyze_interactions(\n",
    "                    W=W,\n",
    "                    nn=nn,\n",
    "                    use_all_interactions=True,\n",
    "                    threshold=1e-5,\n",
    "                    expected_interactions=expected_interactions,\n",
    "                    shell_idx=shell_idx,\n",
    "                    include_dissipation=include_dissipation\n",
    "                )\n",
    "\n",
    "                expected_terms = [entry for entry in significant if entry['status'] == 'expected']\n",
    "                expected_terms.sort(key=lambda x: abs(x['weight']), reverse=True)\n",
    "                \n",
    "                # Compare ranking with expected\n",
    "                learned_ranking = []\n",
    "                for e in expected_terms:\n",
    "                    if e['type'] == 'dissipation':\n",
    "                        learned_ranking.append(('dissipation', e['i']))\n",
    "                    else:\n",
    "                        learned_ranking.append(('interaction', e['i'], e['j'], e['type']))\n",
    "                \n",
    "                expected_ranking = compute_expected_ranking(shell_idx, nn)\n",
    "                ranking_match = learned_ranking == expected_ranking\n",
    "                \n",
    "                shell_data[shell_idx] = {\n",
    "                    'terms': expected_terms,\n",
    "                    'ranking_match': ranking_match\n",
    "                }\n",
    "                max_shell = max(max_shell, shell_idx)\n",
    "\n",
    "    if not shell_data:\n",
    "        print(\" No matching files found for given parameters.\")\n",
    "        return\n",
    "\n",
    "    # Create plot layout\n",
    "    ncols = 5\n",
    "    nrows = int(np.ceil((max_shell + 1) / ncols))\n",
    "    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols * 4.5, nrows * 3.5), constrained_layout=True)\n",
    "\n",
    "    for idx in range(max_shell + 1):\n",
    "        row, col = divmod(idx, ncols)\n",
    "        ax = axs[row][col] if nrows > 1 else axs[col] if ncols > 1 else axs\n",
    "\n",
    "        shell_info = shell_data.get(idx, {'terms': [], 'ranking_match': True})\n",
    "        terms = shell_info['terms']\n",
    "        ranking_match = shell_info['ranking_match']\n",
    "        \n",
    "        labels = [f\"{e['i']},{e['j']} ({e['type']})\" for e in terms]\n",
    "        weights = [abs(e['weight']) for e in terms]\n",
    "\n",
    "        # Classify interaction types\n",
    "        colors = []\n",
    "        for e in terms:\n",
    "            i, j = e['i'] - 1, e['j'] - 1\n",
    "            target = e['target'] - 1\n",
    "            t = e['type']\n",
    "\n",
    "            if t == 'dissipation':\n",
    "                cat = 'dissipation'\n",
    "            elif i > target and j > target:\n",
    "                cat = 'forward-forward'\n",
    "            elif i < target and j < target:\n",
    "                cat = 'backward-backward'\n",
    "            else:\n",
    "                cat = 'backward-forward'\n",
    "\n",
    "            colors.append(type_colors[cat])\n",
    "\n",
    "        # Create bar plot with fixed width for single bars\n",
    "        if len(labels) == 1:\n",
    "            # Single bar case - use fixed width and center it\n",
    "            bar_width = 0.3\n",
    "            x_pos = [0]\n",
    "            ax.bar(x_pos, weights, width=bar_width, color=colors, align='center')\n",
    "            ax.set_xlim(-0.5, 0.5)\n",
    "            ax.set_xticks([0])\n",
    "            ax.set_xticklabels(labels, fontsize=8)\n",
    "        else:\n",
    "            # Multiple bars case - use default behavior\n",
    "            ax.bar(labels, weights, color=colors)\n",
    "            ax.set_xticks(range(len(labels)))\n",
    "            ax.set_xticklabels(labels, rotation=45, ha='right', fontsize=8)\n",
    "        \n",
    "        # Add ranking match indicator to title\n",
    "        ranking_indicator = \"\" if ranking_match else \"\"\n",
    "        title_color = 'green' if ranking_match else 'red'\n",
    "        ax.set_title(f\"Shell {idx + 1} {ranking_indicator}\", color=title_color, fontweight='bold')\n",
    "        ax.set_ylabel(\"Magnitude\")\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Turn off empty subplots\n",
    "    for empty_idx in range(max_shell + 1, nrows * ncols):\n",
    "        row, col = divmod(empty_idx, ncols)\n",
    "        axs[row][col].axis('off')\n",
    "\n",
    "    # Global legend\n",
    "    handles = [plt.Line2D([0], [0], marker='o', color='w', label=key,\n",
    "                          markerfacecolor=color, markersize=10)\n",
    "               for key, color in type_colors.items()]\n",
    "    fig.legend(handles=handles, loc='upper center', ncol=len(type_colors), title=\"Interaction Types\")\n",
    "\n",
    "    # Save plot\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Build filename with nbatch and minocc if specified\n",
    "    nbatch_str = f\"_nbatch{nbatch}\" if nbatch is not None else \"\"\n",
    "    minocc_str = f\"_minocc{minocc}\" if minocc is not None else \"\"\n",
    "    \n",
    "    plot_filename = (f\"expected_terms_{refined}_tstart{t_start}{nbatch_str}{minocc_str}_nn{nn}_\"\n",
    "                     f\"lambda{(lambda_reg if not use_lassoCV else '')}_\"\n",
    "                     f\"lassoCV{use_lassoCV}_subtractD{subtract_dissipation}_knorm{normalize_by_k}.png\")\n",
    "    output_path = os.path.join(output_dir, plot_filename)\n",
    "\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\" Plot saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eb4a27e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Plot saved to /home/vale/SABRA/params_bin2/rankings/expected_terms__tstart1_nbatch5_minocc5_nn20_lambda_lassoCVTrue_subtractDFalse_knormFalse.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "include_dissipation = True  # Whether to include dissipation terms\n",
    "nn=20\n",
    "save_dir='/home/vale/SABRA/params_bin2/Ws_CV'\n",
    "output_path='/home/vale/SABRA/params_bin2/rankings'\n",
    "n_batch=5\n",
    "min_occ=5\n",
    "\n",
    "expected_interactions= get_expected_interactions(nn=nn)\n",
    "\n",
    "\n",
    "plot_expected_interactions_by_shell(nn=nn,\n",
    "    refined=\"\",\n",
    "    t_start=1,\n",
    "    normalize_by_k=False,\n",
    "    subtract_dissipation= not include_dissipation,\n",
    "    use_lassoCV=True,\n",
    "    lambda_reg=5e2,\n",
    "    expected_interactions=expected_interactions,  # your dict of expected terms\n",
    "    include_dissipation=include_dissipation,\n",
    "    nbatch=n_batch,  # Number of batches to filter files\n",
    "    minocc=min_occ,  # Minimum occurrence to filter files\n",
    "    save_dir=save_dir,  # Directory to save plots\n",
    "    output_dir=output_path,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737ab424",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
